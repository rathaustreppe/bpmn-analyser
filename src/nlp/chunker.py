from typing import List, Tuple, Optional

import nltk
from pedantic import pedantic_class


@pedantic_class
class Chunker:
    """
    A class that contains logic to preprocess text and find chunks of text.
    """

    def __init__(self, chunk_grams: str = '',
                 tagged_words_bypass: Optional[
                     List[Tuple[str, str]]] = None) -> None:
        self.grammars = chunk_grams
        self.tagged_words_bypass = tagged_words_bypass
        if chunk_grams == '':
            self._set_init_grammars()

    def _set_init_grammars(self) -> None:
        """
        A function containing all predefined chunk-grammars.
        (in later development they will put in a seperate
        file that is generated by a GUI-Tool.)
        """
        # when defining multiple chunk-grammars, make sure
        # they have no space before them (except tab-spaces) - and are correctly
        # aligned with tabulator
        self.grammars = r"""
        VB_NN_TO_NN:    {<VB.?>+<NN.?><TO><NN.?>+}
        NN_VB_NN:       {<NN.?><VB.?><NN.?>}
        """

    def find_chunk(self, text: str) -> nltk.tree.Tree:
        text = self.preprocess(text=text)
        parser = nltk.RegexpParser(self.grammars)
        parsed = parser.parse(text)

        # S is the root terminal of every tree per default by NLTK.
        # So tt is the root of every chunk. And if no chunk was found,
        # then it is the root of the text.
        # So to decide if sth. is a real chunk, it has a different label
        # then 'S' and is a tree.
        chunks = [chunk for chunk in parsed if
                  isinstance(chunk, nltk.tree.Tree)
                  and chunk.label() != 'S']

        if len(chunks) == 0:
            raise ValueError(
                f'no chunk in text {text} found. Used grammars: {self.grammars}')
        elif len(chunks) > 1:
            raise ValueError(f'{len(chunks)} chunks found in text {text},'
                             f'instead of only one. Found chunks: {chunks}')

        return chunks[0]

    def preprocess(self, text: str) -> List[Tuple[str, str]]:
        # Sentence Tokenizing.
        # from 'long sentence1. long sentence' to
        # [long sentence1', 'long sentence2']
        sentences = nltk.sent_tokenize(text)

        # requirement - constraint: only one sentence is allowed.
        sentence = sentences[0]

        # Word Tokenizing
        # from ['long sentence1'] to ['long', 'sentence1']
        sentence = nltk.wordpunct_tokenize(sentence)

        # Lowercase Normalisation
        # from ["Long", "Sentence"] to ["long", "sentence"]
        # sentence = [word.lower() for word in sentence]

        # Optional: Stop word removal
        # # from [["long", "to", "bar"]] to [["long", "bar"]]
        # def stopword_removal(sentence):
        #     return [word for word in sentence if word not in stopwords]
        # sentences = [stopword_removal(sent) for sent in sentences]

        # POS Tagging
        tagged_sentence = nltk.pos_tag(sentence)

        # # Optional: Lemmatize   - might be useful for more advanced
        # # nlp in near future. So I leave it here.
        # # from [["longer", "bar"]] to [["long", "bar"]]
        # def lemmatize_sentence(sentence, lemmatizer):
        #     return [lemmatizer.lemmatize(word=word) for word in sentence]
        # lemmatizer = nltk.stem.WordNetLemmatizer()
        # sentences= [lemmatize_sentence(sent,lemmatizer) for sent in sentences]

        # sadly there are words, that a pos tagger always classifies wrong.
        # We correct them here.
        def check_bypass(tuple_to_check: Tuple[str, str]) -> Tuple[str, str]:
            if self.tagged_words_bypass is not None:
                for bypass_tuple in self.tagged_words_bypass:
                    if tuple_to_check[0] == bypass_tuple[0]:
                        return tuple_to_check[0], bypass_tuple[1]
            return tuple_to_check

        for idx, tagged_word in enumerate(tagged_sentence):
            tagged_sentence[idx] = check_bypass(tuple_to_check=tagged_word)

        return tagged_sentence

    # def extract_nn_vb_nn(chunk: nltk.tree.Tree, sample_word: str):
    # verb = chunk.leaves()[0]
    # wordtype = get_wordnet_pos(verb[1])
    #
    # word = verb[0]
    # word = wordnet.synsets(word, wordtype)[0]
    # vb_lemmas = set([lemma.name() for lemma in word.lemmas()])
    #
    # word_lemmas = list()
    # for synset in wordnet.synsets(sample_word, wordtype):
    #     for lemma in synset.lemmas():
    #         word_lemmas.append(lemma.name())
    # word_lemmas = set(word_lemmas)
    #
    # intersection_synonyms = word_lemmas.intersection(vb_lemmas)
    # print(f'Chunk-Verb in Text: {vb_lemmas}')
    # print(f'LÃ¶sungs-Synonyme: {word_lemmas}')
    # print(f'union synonyms: {intersection_synonyms}')
    # if len(intersection_synonyms) > 0:
    #     print(f'ACCEPTED: {word}')
    # else:
    #     print(f'REJECTED: {word}')

    #  if isinstance(chunk, nltk.tree.Tree):
    #     if chunk.label() == 'VB_NN_TO_NN':
    #         # chunk.draw()
    #         print('1')
    #     if chunk.label() == 'NN_VB_NN':
    #         print('2')

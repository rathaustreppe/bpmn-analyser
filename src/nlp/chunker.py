from typing import List, Tuple, Optional

import nltk
from pedantic import pedantic_class

from src.exception.language_processing_errors import NoChunkFoundError, \
    MultipleChunksFoundError


@pedantic_class
class Chunker:
    """
    A class that contains logic to preprocess text and find chunks of text.
    """

    def __init__(self, chunk_grams: str = '',
                 tagged_words_bypass: Optional[
                     List[Tuple[str, str]]] = None) -> None:
        self.grammars = chunk_grams
        self.tagged_words_bypass = tagged_words_bypass
        if chunk_grams == '':
            self._set_init_grammars()

    def _set_init_grammars(self) -> None:
        """
        A function containing all predefined chunk-grammars.
        (in later development they will put in a seperate
        file that is generated by a GUI-Tool.)
        """
        # when defining multiple chunk-grammars, make sure
        # they have no space before them (except tab-spaces) - and are correctly
        # aligned with tabulator
        self.grammars = r"""
        VB_NN_TO_NN:    {<VB.?>+<NN.?><TO><NN.?>+}
        NN_VB_NN:       {<NN.?><VB.?><NN.?>}
        """

    def find_chunk(self, text: str) -> nltk.tree.Tree:
        preprocessed_text = self.preprocess(text=text)
        parser = nltk.RegexpParser(self.grammars)
        parsed = parser.parse(preprocessed_text)

        # S is the root terminal of every tree per default by NLTK.
        # So tt is the root of every chunk. And if no chunk was found,
        # then it is the root of the text.
        # So to decide if sth. is a real chunk, it has a different label
        # then 'S' and is a tree.
        chunks = [chunk for chunk in parsed if
                  isinstance(chunk, nltk.tree.Tree)
                  and chunk.label() != 'S']

        if len(chunks) == 0:
            raise NoChunkFoundError(pos_list=preprocessed_text, chunker=self)

        elif len(chunks) > 1:
            raise MultipleChunksFoundError(pos_list=preprocessed_text,
                                           found_chunks=chunks,
                                           chunker=self)
        else:
            return chunks[0]

    def preprocess(self, text: str) -> List[Tuple[str, str]]:
        # Sentence Tokenizing.
        # from 'long sentence1. long sentence' to
        # [long sentence1', 'long sentence2']
        sentences = nltk.sent_tokenize(text)

        # requirement - constraint: only one sentence is allowed.
        if len(sentences) == 0:
            raise ValueError('text with no length detected')
        sentence = sentences[0]

        # Word Tokenizing
        # from ['long sentence1'] to ['long', 'sentence1']
        sentence = nltk.wordpunct_tokenize(sentence)

        # Lowercase Normalisation
        # from ["Long", "Sentence"] to ["long", "sentence"]
        # sentence = [word.lower() for word in sentence]

        # Optional: Stop word removal
        # # from [["long", "to", "bar"]] to [["long", "bar"]]
        # def stopword_removal(sentence):
        #     return [word for word in sentence if word not in stopwords]
        # sentences = [stopword_removal(sent) for sent in sentences]

        # POS Tagging
        tagged_sentence = nltk.pos_tag(sentence)

        # # Optional: Lemmatize   - might be useful for more advanced
        # # nlp in near future. So I leave it here.
        # # from [["longer", "bar"]] to [["long", "bar"]]
        # def lemmatize_sentence(sentence, lemmatizer):
        #     return [lemmatizer.lemmatize(word=word) for word in sentence]
        # lemmatizer = nltk.stem.WordNetLemmatizer()
        # sentences= [lemmatize_sentence(sent,lemmatizer) for sent in sentences]

        # sadly there are words, that a pos tagger always classifies wrong.
        # We correct them here.
        def check_bypass(tuple_to_check: Tuple[str, str]) -> Tuple[str, str]:
            if self.tagged_words_bypass is not None:
                for bypass_tuple in self.tagged_words_bypass:
                    if tuple_to_check[0] == bypass_tuple[0]:
                        return tuple_to_check[0], bypass_tuple[1]
            return tuple_to_check

        for idx, tagged_word in enumerate(tagged_sentence):
            tagged_sentence[idx] = check_bypass(tuple_to_check=tagged_word)

        return tagged_sentence

from typing import List, Tuple

import nltk
from pedantic import pedantic_class


@pedantic_class
class Chunker:
    """
    A class that contains logic to preprocess text and find chunks of text.
    A class cont-aining all predefined chunk-grammars (in later development they
    will put in a seperate file that is auto-generated by a GUI)
    """

    def __init__(self, chunk_grams: str = '') -> None:
        self.grammars = chunk_grams
        self._set_init_grammars()

    def _set_init_grammars(self) -> None:
        # when defining multiple chunk-grammars, make sure
        # they have no space before them (except tab-spaces) - and are correctly
        # aligned with tabulator
        self.grammars = r"""
        NN_VB_NN:       {<NN.?>+<VB.?><TO>?<DT>?<NN.?>+}
        VB_NN_TO_NN:    {<VB.?>+<NN.?><TO>?<NN.?>+}
        """

    def find_chunk(self, text: str) -> nltk.tree.Tree:
        text = self.preprocess(text=text)

        # requirement - constraint: only one sentence is allowed.
        text = nltk.tree.Tree(node=[], children=text[0])

        parser = nltk.RegexpParser(self.grammars)
        chunk = parser.parse(text)
        for chunk in chunk:
            if isinstance(chunk, nltk.tree.Tree):
                if chunk.label() == 'VB_NN_TO_NN':
                    # chunk.draw()
                    print('1')
                if chunk.label() == 'NN_VB_NN':
                    print('2')
        return chunk

    @staticmethod
    def preprocess(text: str) -> List[List[Tuple[str, str]]]:
        # Sentence Tokenizing.
        # from 'long sentence1. long sentence' to
        # [long sentence1', 'long sentence2']
        sentences = nltk.sent_tokenize(text)

        # Word Tokenizing
        # from [long sentence1', 'long sentence2'] to
        # [['long', 'sentence1'], ['long', 'sentence2']]
        sentences = [nltk.wordpunct_tokenize(sentence) for
                     sentence in sentences]

        # Lowercase Normalisation
        # from [["Long", "Sentence"]] to [["long", "sentence"]]
        def lower_sentence(sentence):
            return [word.lower() for word in sentence]

        sentences = [lower_sentence(sent) for sent in
                     sentences]

        # Optional: Stop word removal
        # # from [["long", "to", "bar"]] to [["long", "bar"]]
        # def stopword_removal(sentence):
        #     return [word for word in sentence if word not in stopwords]
        # sentences = [stopword_removal(sent) for sent in sentences]

        # POS Tagging
        sentences = [nltk.pos_tag(sent) for sent in
                     sentences]

        # # Optional: Lemmatize   - might be useful for more advanced
        # # nlp in near future. So I leave it here.
        # # from [["longer", "bar"]] to [["long", "bar"]]
        # def lemmatize_sentence(sentence, lemmatizer):
        #     return [lemmatizer.lemmatize(word=word) for word in sentence]
        # lemmatizer = nltk.stem.WordNetLemmatizer()
        # sentences= [lemmatize_sentence(sent,lemmatizer) for sent in sentences]

        print(f'Preprocessed sentences:', sentences)
        return sentences

    # def extract_nn_vb_nn(chunk: nltk.tree.Tree, sample_word: str):
    # verb = chunk.leaves()[0]
    # wordtype = get_wordnet_pos(verb[1])
    #
    # word = verb[0]
    # word = wordnet.synsets(word, wordtype)[0]
    # vb_lemmas = set([lemma.name() for lemma in word.lemmas()])
    #
    # word_lemmas = list()
    # for synset in wordnet.synsets(sample_word, wordtype):
    #     for lemma in synset.lemmas():
    #         word_lemmas.append(lemma.name())
    # word_lemmas = set(word_lemmas)
    #
    # intersection_synonyms = word_lemmas.intersection(vb_lemmas)
    # print(f'Chunk-Verb in Text: {vb_lemmas}')
    # print(f'LÃ¶sungs-Synonyme: {word_lemmas}')
    # print(f'union synonyms: {intersection_synonyms}')
    # if len(intersection_synonyms) > 0:
    #     print(f'ACCEPTED: {word}')
    # else:
    #     print(f'REJECTED: {word}')
    #
